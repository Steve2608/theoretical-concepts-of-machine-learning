{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Assignment 6: Gradient Descent</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Machine Learning: Theoretical Concepts, SS 2019</h2>\n",
    "\n",
    "<h3 style=\"color:rgb(0,120,170)\">Deadline: see Moodle</h3>\n",
    "Return this notebook with your code and answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconsider assignment 4 where you used gradient descent for logistic regression. Now we are going to compare different methods of adjusting the learning rate for logistic regression. For this you can either use the implementation provided in this notebook or your own implementation from assignment 4. If you use your own implementation, make sure to add a bias/intercept unit to your model!\n",
    "\n",
    "For the following tasks use the dataset `dataset-cf10-46.csv` that consists of $32 \\times 32$ pixel images of cats and dogs. Note that the first column is the class label, the remaining columns contain the gray scale pixel intensities. If you want to visualize the data, you can do this using the\n",
    "following function `plot_image`.\n",
    "\n",
    "Split the dataset into 50% test set, 30% training set and 20% validation set. Use the validation set to find optimal hyperparameters in some of the subproblems of this exercise. Run your program always for 5000 full-batch iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:39:59.709209Z",
     "start_time": "2019-05-15T21:39:52.098576Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def logistic_gradient(w, x, y):\n",
    "    sigma = 1.0 / (1.0 + np.exp(-np.matmul(w, x)))\n",
    "    return np.sum(x * (sigma - y), 1)\n",
    "\n",
    "\n",
    "def cost(w, x, y):\n",
    "    sigma = 1.0 / (1.0 + np.exp(-np.matmul(w, x)))\n",
    "    return np.sum(-y * np.log(sigma) - (1.0 - y) * (np.log(1.0 - sigma)))\n",
    "\n",
    "\n",
    "def fitLogRegModel(x, y, eta, epochs=5000, log_interval=500, momentum=None, verbose=False):\n",
    "    x = x.T\n",
    "    w = np.random.uniform(-0.1, 0.1, size=[x.shape[0] + 1])\n",
    "    # no previous delta\n",
    "    d_wn = 0\n",
    "    x = np.concatenate([x, np.ones((1, x.shape[1]))], axis=0)\n",
    "    c = cost(w, x, y)\n",
    "    if verbose:\n",
    "        print(\"inital cost: \", c)\n",
    "    for i in range(epochs):\n",
    "        if momentum:\n",
    "            d_wn = -eta * logistic_gradient(w, x, y) + momentum * d_wn\n",
    "            w += d_wn\n",
    "        else:\n",
    "            w -= eta * logistic_gradient(w, x, y)\n",
    "        c = cost(w, x, y)\n",
    "        if verbose and (i + 1) % log_interval == 0:\n",
    "            print(c)\n",
    "    if verbose:\n",
    "        print(\"final cost: \", c)\n",
    "    return w\n",
    "\n",
    "\n",
    "def predictLogReg(w, xPred):\n",
    "    xPred = np.concatenate([xPred, np.ones((1, xPred.shape[1]))], axis=0)\n",
    "    return 1.0 / (1.0 + np.exp(-np.matmul(w, xPred)))\n",
    "\n",
    "\n",
    "def plot_image(row):\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(np.reshape(row, (32, 32)), cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "data = np.genfromtxt('dataset-cf10-46.csv', delimiter=',', skip_header=True)\n",
    "data_len = data.shape[0]\n",
    "test_len = int(data_len * 0.5)\n",
    "train_len = int(data_len * 0.3)\n",
    "np.random.shuffle(data)\n",
    "test_data = data[:test_len, 1:]\n",
    "test_labels = (data[:test_len, 0] == 4.).astype(np.float)\n",
    "train_data = data[test_len:(test_len + train_len), 1:]\n",
    "train_labels = (data[test_len:(test_len + train_len), 0] == 4.).astype(\n",
    "    np.float)\n",
    "val_data = data[(test_len + train_len):, 1:]\n",
    "val_labels = (data[(test_len + train_len):, 0] == 4.).astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:39:59.717139Z",
     "start_time": "2019-05-15T21:39:59.712173Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_val = train_data, val_data\n",
    "X_test, y_train = test_data, train_labels\n",
    "y_val, y_test = val_labels, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Try to find a constant learning rate $\\eta$ that gives you good results for the classification task described below. To do this, try at least 3 different choices for $\\eta$. (30P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:39:59.740603Z",
     "start_time": "2019-05-15T21:39:59.722657Z"
    }
   },
   "outputs": [],
   "source": [
    "def line_search_eta(X_train, X_test, y_train, y_test, etas):\n",
    "    opt = None\n",
    "    X = np.vstack((X_test.T, np.ones(X_test.shape[0])))\n",
    "\n",
    "    for eta in etas:\n",
    "        w = fitLogRegModel(X_train, y_train, eta)\n",
    "        c = cost(w, X, y_test)\n",
    "        if opt is None or c < opt['cost']:\n",
    "            print('New optimal 洧랙 found!')\n",
    "            print(f'Old 洧랙 was: {opt}')\n",
    "            opt = {'w': w, '洧랙': eta, 'cost': c}\n",
    "            print(f'New optimal 洧랙: {opt}\\n')\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:39:59.748583Z",
     "start_time": "2019-05-15T21:39:59.743596Z"
    }
   },
   "outputs": [],
   "source": [
    "start_eta = -10\n",
    "end_eta = -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:39:59.761560Z",
     "start_time": "2019-05-15T21:39:59.754571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e-10 3.59381366e-10 1.29154967e-09 4.64158883e-09\n",
      " 1.66810054e-08 5.99484250e-08 2.15443469e-07 7.74263683e-07\n",
      " 2.78255940e-06 1.00000000e-05]\n"
     ]
    }
   ],
   "source": [
    "steps_eta = (end_eta - start_eta) * 2\n",
    "etas = np.logspace(start_eta, end_eta, steps_eta)\n",
    "print(etas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:50:52.564796Z",
     "start_time": "2019-05-15T21:39:59.763543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New optimal 洧랙 found!\n",
      "Old 洧랙 was: None\n",
      "New optimal 洧랙: {'w': array([ 0.05410822, -0.00575195,  0.08416298, ..., -0.08703995,\n",
      "        0.03416205, -0.06006317]), '洧랙': 1e-10, 'cost': 1026.1210263392045}\n",
      "\n",
      "New optimal 洧랙 found!\n",
      "Old 洧랙 was: {'w': array([ 0.05410822, -0.00575195,  0.08416298, ..., -0.08703995,\n",
      "        0.03416205, -0.06006317]), '洧랙': 1e-10, 'cost': 1026.1210263392045}\n",
      "New optimal 洧랙: {'w': array([ 0.08426172, -0.06904278,  0.02355034, ...,  0.03272793,\n",
      "       -0.09841147, -0.077335  ]), '洧랙': 3.593813663804626e-10, 'cost': 738.9414488814813}\n",
      "\n",
      "New optimal 洧랙 found!\n",
      "Old 洧랙 was: {'w': array([ 0.08426172, -0.06904278,  0.02355034, ...,  0.03272793,\n",
      "       -0.09841147, -0.077335  ]), '洧랙': 3.593813663804626e-10, 'cost': 738.9414488814813}\n",
      "New optimal 洧랙: {'w': array([-0.00164597, -0.06558004, -0.01751935, ..., -0.00682596,\n",
      "       -0.05837557,  0.0903722 ]), '洧랙': 4.641588833612773e-09, 'cost': 712.2335166785931}\n",
      "\n",
      "New optimal 洧랙 found!\n",
      "Old 洧랙 was: {'w': array([-0.00164597, -0.06558004, -0.01751935, ..., -0.00682596,\n",
      "       -0.05837557,  0.0903722 ]), '洧랙': 4.641588833612773e-09, 'cost': 712.2335166785931}\n",
      "New optimal 洧랙: {'w': array([-0.01712088, -0.09018632,  0.05903724, ..., -0.08790942,\n",
      "        0.03003027, -0.06541856]), '洧랙': 1.6681005372000592e-08, 'cost': 708.9599571240798}\n",
      "\n",
      "New optimal 洧랙 found!\n",
      "Old 洧랙 was: {'w': array([-0.01712088, -0.09018632,  0.05903724, ..., -0.08790942,\n",
      "        0.03003027, -0.06541856]), '洧랙': 1.6681005372000592e-08, 'cost': 708.9599571240798}\n",
      "New optimal 洧랙: {'w': array([-0.03414268,  0.09156366, -0.08355223, ...,  0.07808764,\n",
      "       -0.0122058 ,  0.01420335]), '洧랙': 5.99484250318941e-08, 'cost': 696.2439999875094}\n",
      "\n",
      "New optimal 洧랙 found!\n",
      "Old 洧랙 was: {'w': array([-0.03414268,  0.09156366, -0.08355223, ...,  0.07808764,\n",
      "       -0.0122058 ,  0.01420335]), '洧랙': 5.99484250318941e-08, 'cost': 696.2439999875094}\n",
      "New optimal 洧랙: {'w': array([ 0.01957259,  0.01390511,  0.08118045, ...,  0.00264623,\n",
      "        0.00759464, -0.07776009]), '洧랙': 2.1544346900318867e-07, 'cost': 681.261877343735}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_eta = line_search_eta(X_train, X_val, y_train, y_val, etas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "Use a constant learning rate that worked well in task 1 and add a momentum term $\\mu$. Try at least 3 different choices for $\\mu$. (30P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:50:52.574769Z",
     "start_time": "2019-05-15T21:50:52.566791Z"
    }
   },
   "outputs": [],
   "source": [
    "def line_search_mu(X_train, X_test, y_train, y_test, eta, mus):\n",
    "    opt = None\n",
    "    X = np.vstack((X_test.T, np.ones(X_test.shape[0])))\n",
    "\n",
    "    for mu in mus:\n",
    "        w = fitLogRegModel(X_train, y_train, eta, momentum=mu)\n",
    "        c = cost(w, X, y_test)\n",
    "        if opt is None or c < opt['cost']:\n",
    "            print('New optimal 洧랞 found!')\n",
    "            print(f'Old 洧랞 was: {opt}')\n",
    "            opt = {'w': w, '洧랙': eta, '洧랞': mu, 'cost': c}\n",
    "            print(f'New optimal 洧랞: {opt}\\n')\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:50:52.589773Z",
     "start_time": "2019-05-15T21:50:52.577284Z"
    }
   },
   "outputs": [],
   "source": [
    "start_mom = -10\n",
    "end_mom = -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T21:50:52.597765Z",
     "start_time": "2019-05-15T21:50:52.591761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e-10 3.59381366e-10 1.29154967e-09 4.64158883e-09\n",
      " 1.66810054e-08 5.99484250e-08 2.15443469e-07 7.74263683e-07\n",
      " 2.78255940e-06 1.00000000e-05]\n"
     ]
    }
   ],
   "source": [
    "steps_mom = (end_mom - start_mom) * 2\n",
    "moms = np.logspace(start_mom, end_mom, steps_mom)\n",
    "print(moms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:01:39.351675Z",
     "start_time": "2019-05-15T21:50:52.601744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New optimal 洧랞 found!\n",
      "Old 洧랞 was: None\n",
      "New optimal 洧랞: {'w': array([-0.03929086, -0.03699061,  0.03452047, ..., -0.05161224,\n",
      "        0.06582762,  0.06225613]), '洧랙': 2.1544346900318867e-07, '洧랞': 1e-10, 'cost': 690.3113335927104}\n",
      "\n",
      "New optimal 洧랞 found!\n",
      "Old 洧랞 was: {'w': array([-0.03929086, -0.03699061,  0.03452047, ..., -0.05161224,\n",
      "        0.06582762,  0.06225613]), '洧랙': 2.1544346900318867e-07, '洧랞': 1e-10, 'cost': 690.3113335927104}\n",
      "New optimal 洧랞: {'w': array([-0.02386941,  0.01781461,  0.02650048, ...,  0.01493064,\n",
      "        0.05297607, -0.00687054]), '洧랙': 2.1544346900318867e-07, '洧랞': 1.2915496650148826e-09, 'cost': 684.6556795140409}\n",
      "\n",
      "New optimal 洧랞 found!\n",
      "Old 洧랞 was: {'w': array([-0.02386941,  0.01781461,  0.02650048, ...,  0.01493064,\n",
      "        0.05297607, -0.00687054]), '洧랙': 2.1544346900318867e-07, '洧랞': 1.2915496650148826e-09, 'cost': 684.6556795140409}\n",
      "New optimal 洧랞: {'w': array([ 0.02155808, -0.0673873 ,  0.02756168, ...,  0.0745055 ,\n",
      "        0.04369904, -0.0293662 ]), '洧랙': 2.1544346900318867e-07, '洧랞': 4.641588833612773e-09, 'cost': 682.8322448228108}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eta = best_eta['洧랙']\n",
    "best_mu = line_search_mu(X_train, X_val, y_train, y_val, eta, moms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "Implement line search and use it to find the optimal learning rate in each update iteration. Explain how you got to your solution. (40P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:01:39.364638Z",
     "start_time": "2019-05-15T22:01:39.355661Z"
    }
   },
   "outputs": [],
   "source": [
    "def poly_eta(w, g, X, y):\n",
    "    # using the knowledge from Task 1\n",
    "    a = 1e-4\n",
    "    b = 1e-10\n",
    "    c = 1e-5\n",
    "\n",
    "    # just some impossible value\n",
    "    d = -1\n",
    "\n",
    "    # very small epsilon\n",
    "    while a - b > 1e-14:\n",
    "        cost_a = cost(w - a * g, X, y)\n",
    "        cost_b = cost(w - b * g, X, y)\n",
    "        cost_c = cost(w - c * g, X, y)\n",
    "\n",
    "        # create a polynomial\n",
    "        poly = np.poly1d(np.polyfit((a, b, c), (cost_a, cost_b, cost_c),\n",
    "                                    deg=2))\n",
    "        # take the derivative\n",
    "        derivative = np.polyder(poly)\n",
    "        # get its argument\n",
    "        d = derivative.roots[0]\n",
    "\n",
    "        a, b, c = a if cost_a < cost_b else b, c, d\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:02:01.719549Z",
     "start_time": "2019-05-15T22:02:01.713255Z"
    }
   },
   "outputs": [],
   "source": [
    "def fitLogRegModelLine(X_train, y_train, epochs=5000, log_interval=500):\n",
    "    w = np.random.uniform(-0.1, 0.1, size=X_train.shape[1] + 1)\n",
    "    X = np.vstack((X_train.T, np.ones(X_train.shape[0])))\n",
    "\n",
    "    for i in range(epochs):\n",
    "        g = logistic_gradient(w, X, y_train)\n",
    "        eta = poly_eta(w, g, X, y_train)\n",
    "        w -= eta * g\n",
    "    return {'w': w, '洧랙': eta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T22:03:38.111063Z",
     "start_time": "2019-05-15T22:02:01.838246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w': array([ 0.03191847,  0.09928177,  0.24294392, ..., -0.18123043,\n",
      "       -0.33154009,  0.50982313]), '洧랙': 0.000119470774939898}\n"
     ]
    }
   ],
   "source": [
    "print(fitLogRegModelLine(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3 is very similar to Task 1; the difference being that now we do not use a one-size-fits all learning rate $\\eta$ but a dynamically calculated one for each iteration. \n",
    "\n",
    "For this we use a line-search for $\\eta$ in $[10^{-10}, 10^{-4}]$. In a binary-search-process we then subsequently fit 3 interpolation points to a polynomial of second degree, reducing the search space each iteration (see slide 6) until the boundaries of the search space are closer than $\\epsilon = 10^{-14}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Also the biggest learning rate for which given functions wouldn't end up overflowing was around $10^{-4}$ which made it the upper boundary for technical reasons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
