{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Assignment 3: Expectation Maximization</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Machine Learning: Theoretical Concepts, SS 2019</h2>\n",
    "\n",
    "<h3 style=\"color:rgb(0,120,170)\">Deadline April 09th 2019, 24:00</h3>\n",
    "Return this notebook with your code and answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "<h3 style=\"color:rgb(0,120,170)\"> Question 3.1 </h3>\n",
    "\n",
    "This question is meant to give you some more theoretical insights into the EM algorithm.\n",
    "As you have seen in the lecture, when doing EM you improve the log likelihood of the data $\\sum_i^N \\mathsf{log} p(\\mathbf{x}^i\\mid \\mathbf{\\Theta})$ by introducing a distribution $Q(\\mathbf{u})$ on hidden variables $\\mathbf{u}$. Then, instead of maximizing the log likelihood directly, you maximize a lower bound which is obtained through Jensen's inequality. We have:\n",
    "\\begin{align}\n",
    "\\sum_i^N \\mathsf{log} p(\\mathbf{x}^i) \\geq \\sum_i^N \\int Q(\\mathbf{u}) \\mathsf{log} \\frac{p(\\mathbf{x}^i, \\mathbf{u})}{Q(\\mathbf{u})}d\\mathbf{u} \\ .\n",
    "\\end{align}\n",
    "Show that after each E-step in the EM algorithm, this lower bound is reached with equality (\"the bound is tight\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <span style=\"color:rgb(0,120,170)\">**TODO:** Markdown, Latex</span>\n",
    "\n",
    "\n",
    "## Assumptions \n",
    "According to lecture slides: \n",
    "\\begin{align*}\n",
    "\\ln \\mathcal{L}(\\{x \\}; \\mathcal{w}) \\ge \\mathcal{F}(\\mathcal{Q}, \\mathcal{w})\n",
    "\\end{align*}\n",
    "\n",
    "And we want to show (tightness of bound): \n",
    "\\begin{align*}\n",
    "\\ln \\mathcal{L}(\\{x \\}; \\mathcal{w}) = \\mathcal{F}(\\mathcal{Q}, \\mathcal{w})\n",
    "\\end{align*}\n",
    "holds after $k$ steps: \n",
    "\n",
    "\\begin{align*}\n",
    "\\ln \\mathcal{L}(\\{x \\}; \\mathcal{w_k}) &= \\mathcal{F}(\\mathcal{Q_{k+1}}, \\mathcal{w_k}) \\\\\n",
    "\\ln \\mathcal{L}(\\{x \\}; \\mathcal{w_k}) &= \\mathcal{F}(\\mathcal{Q_{k+1}}(\\mathcal{u} ~|~ \\{ x \\}); \\mathcal{w_k}) \\\\\n",
    "\\\\\n",
    "\\mathcal{Q_{k+1}}(\\mathcal{u} ~|~ \\{ \\mathcal{x} \\}) &= \\mathcal{p}(\\mathcal{u} ~|~ \\{ \\mathcal{x} \\}; \\mathcal{w}_k)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation \n",
    "\n",
    "\\begin{align*}\n",
    "\\ln \\mathcal{L}(\\{x \\}; \\mathcal{w_k}) &= \\mathcal{F}(\\mathcal{Q_k}(\\mathcal{u} ~|~ \\{ \\mathcal{x} \\}); \\mathcal{w_k}) \\\\\n",
    "&= \\int_U \\mathcal{Q_k}(\\mathcal{u} ~|~ \\{ \\mathcal{x} \\}) \\ln \\frac{\\mathcal{p} (\\{ \\mathcal{x} \\}, \\mathcal{u}; \\mathcal{w_k})}{\\mathcal{Q}(\\mathcal{u} ~|~ \\{ \\mathcal{x} \\})} \\delta \\mathcal{u} \\\\\n",
    "&= \\int_U \\mathcal{Q_k}(\\mathcal{u} ~|~ \\{ \\mathcal{x} \\}) \\ln \\frac{\\mathcal{p} (\\{ \\mathcal{x} \\}, \\mathcal{u}; \\mathcal{w_k})}{\\mathcal{Q}(\\mathcal{u} ~|~ \\{ \\mathcal{x} \\})} \\delta \\mathcal{u}  + \\ln \\mathcal{p}(\\{ \\mathcal{x} \\}; \\mathcal{w_k}) \\\\\n",
    "&= \\int_U \\mathcal{Q_k}(\\mathcal{u} ~|~ \\{ \\mathcal{x} \\}) \\ln \\frac{\\mathcal{p} (\\{ \\mathcal{x} \\}, \\mathcal{u}; \\mathcal{w_k})}{\\mathcal{Q}(\\mathcal{u} ~|~ \\{ \\mathcal{x} \\})} \\delta \\mathcal{u}  + \\ln \\mathcal{L}(\\{ \\mathcal{x} \\}; \\mathcal{w_k}) \\\\\n",
    "&= - \\int_U \\mathcal{Q_k}(\\mathcal{u} ~|~ \\{ \\mathcal{x} \\}) \\ln \\frac{\\mathcal{Q}(\\mathcal{u} ~|~ \\{ \\mathcal{x} \\})}{\\mathcal{p} (\\{ \\mathcal{x} \\}, \\mathcal{u}; \\mathcal{w_k})} \\delta \\mathcal{u}  + \\ln \\mathcal{L}(\\{ \\mathcal{x} \\}; \\mathcal{w_k}) \\\\\n",
    "&= \\ln \\mathcal{L}(\\{ \\mathcal{x} \\}; \\mathcal{w_k}) - \\underbrace{\\mathcal{D}_{KL}(\\mathcal{Q} || \\mathcal{p})}_\\text{$\\ge 0$; $=0$ for $\\mathcal{Q}(\\mathcal{u} ~|~ \\{\\mathcal{x}\\}) = \\mathcal{p}( \\mathcal{u} ~|~ \\{ \\mathcal{x} \\}; \\mathcal{w_k})$}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this equality holds if and only if $\\mathcal{D}_{KL}(\\mathcal{Q} || \\mathcal{p}) = 0$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{F}(\\mathcal{Q_{k+1}}(\\mathcal{u} ~|~ \\{ x \\}); \\mathcal{w_k}) &= \\ln \\mathcal{L}(\\{x \\}; \\mathcal{w_k}) \\\\\n",
    "&= \\ln \\mathcal{L}(\\{ \\mathcal{x} \\}; \\mathcal{w_k}) - \\mathcal{D}_{KL}(\\mathcal{Q} || \\mathcal{p}) \\\\\n",
    "&= \\ln \\mathcal{L}(\\{ \\mathcal{x} \\}; \\mathcal{w_k}) - \\int_U \\mathcal{Q}_{k+1}(\\mathcal{u} ~|~ \\{ \\mathcal{x} \\}) \\ln \\frac{\\mathcal{Q}_{k+1}(\\mathcal{u} ~|~ \\{ \\mathcal{x} \\})}{\\mathcal{p} (\\{ \\mathcal{x} \\}, \\mathcal{u}; \\mathcal{w_k})} \\delta \\mathcal{u} \\\\\n",
    "&= \\ln \\mathcal{L}(\\{ \\mathcal{x} \\}; \\mathcal{w_k}) - \\int_U \\mathcal{Q}_{k+1}(\\mathcal{u} ~|~ \\{ \\mathcal{x} \\}) \\ln \\frac{\\mathcal{p} (\\{ \\mathcal{x} \\}, \\mathcal{u}; \\mathcal{w_k})}{\\mathcal{p} (\\{ \\mathcal{x} \\}, \\mathcal{u}; \\mathcal{w_k})} \\delta \\mathcal{u} \\\\\n",
    "&= \\ln \\mathcal{L}(\\{ \\mathcal{x} \\}; \\mathcal{w_k}) - \\int_U \\mathcal{Q}_{k+1}(\\mathcal{u} ~|~ \\{ \\mathcal{x} \\}) \\ln 1 \\delta \\mathcal{u} \\\\\n",
    "&= \\ln \\mathcal{L}(\\{ \\mathcal{x} \\}; \\mathcal{w_k}) - \\int_U \\mathcal{Q}_{k+1}(\\mathcal{u} ~|~ \\{ \\mathcal{x} \\}) 0 \\delta \\mathcal{u} \\\\\n",
    "&= \\ln \\mathcal{L}(\\{ \\mathcal{x} \\}; \\mathcal{w_k}) - 0 \\\\\n",
    "&= \\ln \\mathcal{L}(\\{ \\mathcal{x} \\}; \\mathcal{w_k})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\"> Question 3.2 </h3>\n",
    "\n",
    "Derive update formulas for the Mixture of Poissons problem shown in class. This means: update formulas for the $r_{ik}$, $\\alpha_k$ and $\\theta_k$ (the rate parameter) to be used in the E- and M-steps.\n",
    "Note that most of the formulas derived in class for the Mixture of Gaussians problem can be reused, as they are not specific to the Gaussian distribution, but for a Mixture Model in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <span style=\"color:rgb(0,120,170)\">**TODO:** Markdown / Latex</span>\n",
    "\n",
    "First we update the formula for $\\theta_k$:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\ln \\mathcal{L}}{\\partial \\theta_k} &= \\sum_i^n \\frac{\\alpha_k}{\\sum_l^K \\alpha_l \\mathcal{p}(x_i; \\theta_l)} \\frac{\\partial p(x_i; \\theta_k)}{\\partial \\theta_k}\\\\\n",
    "&= \\sum_i^n p(u_i ~=~ k|x_i) \\frac{\\partial \\ln p(x_i; \\theta_k)}{\\partial \\theta_k}\n",
    "\\end{align*}\n",
    "\n",
    "We also know that for the Poisson distribution there is just one parameter to optimize:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\lambda_k} \\ln p(x_i; \\lambda_k) &= \\frac{\\partial}{\\partial \\lambda_k} \\ln \\frac{\\lambda_k^{x_i}}{x_i!} \\mathcal{e}^{-\\lambda_k} \\\\\n",
    "&= \\frac{\\partial}{\\partial \\lambda_k} \\ln(\\frac{\\lambda_k^{x_i}}{x_i!}) - \\lambda_k \\\\\n",
    "&= \\frac{\\partial}{\\partial \\lambda_k} \\ln(\\lambda_k^{x_i}) - \\ln(x_i!) - \\lambda_k \\\\\n",
    "&= \\frac{\\partial}{\\partial \\lambda_k} x_i\\ln(\\lambda_k) - \\ln(x_i!) - \\lambda_k \\\\\n",
    "&= \\frac{x_i}{\\lambda_k} - 1\n",
    "\\end{align*}\n",
    "\n",
    "Setting it to zero gives us:\n",
    "\\begin{align*}\n",
    "\\frac{x_i}{\\lambda_k} - 1 &= 0 \\\\\n",
    "\\frac{x_i}{\\lambda_k} &= 1 \\\\\n",
    "\\lambda_k &= x_i\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the original formula, we now have: \n",
    "\\begin{align*}\n",
    "\\sum_i^n p(u_i ~=~ k|x_i) \\frac{\\partial \\ln p(x_i; \\theta_k)}{\\partial \\theta_k} &= \\sum_i^n p(u_i ~=~ k|x_i) (\\frac{x_i}{\\lambda_k} - 1)\n",
    "\\end{align*}\n",
    "\n",
    "We also set this to 0: \n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_i^n p(u_i ~=~ k|x_i) (\\frac{x_i}{\\lambda_k} - 1) &= 0 \\\\\n",
    "\\sum_i^n r_{ik} (\\frac{x_i}{\\lambda_k} - 1) &= 0 \\\\\n",
    "\\sum_i^n r_{ik} \\frac{x_i}{\\lambda_k} - \\sum_i^n r_{ik} &= 0 \\\\\n",
    "\\sum_i^n r_{ik} x_i - \\sum_i^n r_{ik}\\lambda_k  &= 0 \\\\\n",
    "\\sum_i^n r_{ik} x_i &= \\sum_i^n r_{ik}\\lambda_k \\\\\n",
    "\\lambda_k &= \\frac{\\sum_i^n r_{ik} x_i}{\\sum_i^n r_{ik}} \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lagrangian of the log-likelihood and the sum-to-one constraint (according to exercise slides):\n",
    "\n",
    "\\begin{align*}\n",
    "\\Lambda(\\Theta, \\lambda) = \\sum_i^n \\ln\\big(\\sum_l^K \\alpha_l p(x_i; \\theta_l) \\big) + \\lambda \\big( \\sum_l^K a_l - 1 \\big) \n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\Lambda(\\Theta, \\lambda)}{\\partial \\alpha_k} = \\sum_i^n \\frac{p(x_i; \\theta_k)}{\\sum_l^m \\alpha_l p(x_i; \\theta_l)} + \\lambda &= 0 \\\\\n",
    "\\sum_i^n p(u_i ~=~ k|x_i) + \\alpha_k \\lambda &= 0 \\\\\n",
    "\\sum_i^n \\underbrace{\\sum_k^K p(u_i ~=~ k|x_i)}_\\text{sums up to 1} + \\lambda \\underbrace{\\sum_k^K \\alpha_k}_\\text{sums up to 1} &= 0 \\\\\n",
    "\\sum_i^n 1 &= - \\lambda \\\\\n",
    "\\lambda &= -n \\implies \\alpha_k = \\frac{1}{n} \\sum_i^n p(u_i = k | x_i)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $r_{ik}$:\n",
    "\n",
    "\\begin{align*}\n",
    "r_{ik} &= p(u_i k | x_i) \\\\\n",
    "&= \\frac{p(u_i = k) p(x_i | u_i = k)}{p(x_i)} \\\\\n",
    "&= \\frac{\\alpha_k p(x_i; \\theta_k)}{\\sum_l^K \\alpha_l p(x_i; \\theta_l)} \\\\\n",
    "&= \\frac{\\alpha_k \\frac{\\lambda_k^{x_i}}{x_i!} \\mathcal{e}^{-\\lambda_k}}{\\sum_l^K \\alpha_l \\frac{\\lambda_k^{x_i}}{x_i!} \\mathcal{e}^{-\\lambda_k}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\"> Question 3.3 </h3>\n",
    "\n",
    "\n",
    "Write a program that estimates all the parameters of a Mixture of Poissons via an EM algorithm. Use the derivations from exercise 3.2. Choose (and document!) your own convergence criterion. Then, let your algorithm run on the data from $\\mathsf{cnvdata.csv}$.\n",
    "Try different values for $K$, the number of components.\n",
    "For each $K$, run different experiments with different random initializations.\n",
    "\n",
    "How many different copy numbers (i.e. components or clusters) do you think are in the data? What do you observe, and how do different values of $K$ change the results of your algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <span style=\"color:rgb(0,120,170)\">**TODO:** Markdown / Latex</span>\n",
    "\n",
    "Initially I thought that there are **clearly** 3 underlying poisson distributions (~50, ~90, ~185). Upon further inspection there might be 2 different distributions around X=90."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T14:16:17.990634Z",
     "start_time": "2019-04-09T14:16:17.647461Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.loadtxt(open(\"cnvdata.csv\", \"rb\"), delimiter=\",\", skiprows=1)\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.hist(data, bins=50)\n",
    "plt.title('Data distribution')\n",
    "plt.ylabel('$p(X)$')\n",
    "plt.xlabel('$X$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "* <span style=\"color:rgb(0,120,170)\">**TODO:** Implement your EM algorithm using the updates rules derived in the previous exercise.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T14:11:26.117535Z",
     "start_time": "2019-04-09T14:11:26.102572Z"
    }
   },
   "outputs": [],
   "source": [
    "class EM:\n",
    "    def __init__(self, X, K, max_iter, verbose=False):\n",
    "        self.X = X\n",
    "        self.K = K\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def poisson(self, l, x_i):\n",
    "        return stats.poisson(l).pmf(x_i)\n",
    "\n",
    "    def alpha_optimize(self, R_ik, k) -> np.real:\n",
    "        \"\"\"Optimizing for one alpha in k-th column\"\"\"\n",
    "        return R_ik[:, k].sum() / len(R_ik)\n",
    "\n",
    "    def alphas(self, R_ik) -> [np.real]:\n",
    "        \"\"\"Optimizing over all alphas\"\"\"\n",
    "        return np.asarray([self.alpha_optimize(R_ik, k) for k in range(R_ik.shape[1])])\n",
    "\n",
    "    def lambda_optimize(self, R_ik, X, k) -> np.real:\n",
    "        \"\"\"Optimizing for one lambda_k:\n",
    "        lambda_k = sum(R_ik * x_i) / sum(R_ik)\"\"\"\n",
    "        return (R_ik[:, k] * X).sum() / R_ik[:, k].sum()\n",
    "\n",
    "    def lambdas(self, R_ik, X) -> [np.real]:\n",
    "        \"\"\"Optimizing over all lambdas\"\"\"\n",
    "        return np.asarray([self.lambda_optimize(R_ik, X, k) for k in range(R_ik.shape[1])])\n",
    "\n",
    "    def r_ik_optimize(self, X, i, K, k, L, A) -> [np.real, np.real]:\n",
    "        \"\"\"Optimizing for one r_ik\"\"\"\n",
    "        return A[k] * self.poisson(L[k], X[i]) / np.asarray(\n",
    "            [A[l] * self.poisson(lam, X[i]) for l, lam in zip(range(K), L)]).sum()\n",
    "\n",
    "    def M_step(self, R_ik, X):\n",
    "        return self.alphas(R_ik), self.lambdas(R_ik, X)\n",
    "\n",
    "    def E_step_initial(self):\n",
    "        return np.random.rand(self.X.shape[0], self.K)\n",
    "\n",
    "    def E_step(self, X, K, L, A):\n",
    "        R_ik = np.zeros((X.shape[0], K))\n",
    "        # Iterating over all x_i\n",
    "        for i in range(X.shape[0]):\n",
    "            # iterating over all ks\n",
    "            for k in range(K):\n",
    "                R_ik[i, k] = self.r_ik_optimize(X, i, K, k, L, A)\n",
    "        return R_ik\n",
    "\n",
    "    def does_converge(self, L_curr, L_prev) -> bool:\n",
    "        return np.allclose(L_curr, L_prev, rtol=1e-06, atol=1e-06)\n",
    "\n",
    "    def EM(self):\n",
    "        L_curr = np.zeros(self.K)\n",
    "        R_ik = self.E_step_initial()\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            if self.verbose:\n",
    "                print(f'Iteration {i + 1}/{self.max_iter}')\n",
    "            L_prev = L_curr.copy()\n",
    "            A, L_curr = self.M_step(R_ik, self.X)\n",
    "            R_ik = self.E_step(self.X, self.K, L_curr, A)\n",
    "            if self.does_converge(L_curr, L_prev):\n",
    "                if self.verbose:\n",
    "                    print(f'EM reached convergence in step {i + 1}!')\n",
    "                return A, L_curr\n",
    "\n",
    "        return L_curr, A \n",
    "\n",
    "    def __str__(self):\n",
    "        result = self.EM()\n",
    "        result_as_list = [(l, a) for l, a in zip(result[0], result[1])]\n",
    "        result_sorted = np.asarray(sorted(result_as_list))\n",
    "        return f'Lambdas: {result_sorted[:, 0]}\\nAlphas: {result_sorted[:, 1]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behaviour of the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T14:11:35.422319Z",
     "start_time": "2019-04-09T14:11:26.118533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambdas: [ 48.34256499  93.44427824 186.97168677]\n",
      "Alphas: [0.53414834 0.10217218 0.36367948]\n"
     ]
    }
   ],
   "source": [
    "print(EM(data, 3, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\mathcal{K} = 3$ the algorithm picks up each of the \"bumps\" as was to be expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T14:12:03.336151Z",
     "start_time": "2019-04-09T14:11:35.423307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambdas: [ 48.30901046  59.36037511  88.18334807 102.45873728 109.61549339\n",
      " 187.03072453]\n",
      "Alphas: [0.52767064 0.01714872 0.04088731 0.03888047 0.01220549 0.36320737]\n"
     ]
    }
   ],
   "source": [
    "print(EM(data, 6, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\mathcal{K} = 6$ the algorithm picks up multiple distributions for each visible \"bump\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T14:13:17.172977Z",
     "start_time": "2019-04-09T14:12:03.337361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambdas: [ 47.86718482  49.02583478  54.06497217  88.84626416 100.38275511\n",
      " 107.52577761 174.88253683 183.67873675 186.20316881 190.37242509]\n",
      "Alphas: [0.34011717 0.16926133 0.03492068 0.04004609 0.03348921 0.01864175\n",
      " 0.02504023 0.0667347  0.09445967 0.17728917]\n"
     ]
    }
   ],
   "source": [
    "print(EM(data, 10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\mathcal{K} \\ge 10$ the algorithm tends to overfit every statistical anomality into its own distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for 4 components\n",
    "\n",
    "* <span style=\"color:rgb(0,120,170)\">**TODO:** Test your implementation for 4 components</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first and second distribution (~48.5 and ~187) are almost always picked up for $\\mathcal{K} =4$. The other two distributions fluctuate immensly between adding a second distribution near the first and last distribution and picking up on two distributions in between:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T14:13:27.920738Z",
     "start_time": "2019-04-09T14:13:17.174000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambdas: [ 47.83534053  50.45338699  97.00948568 186.98132198]\n",
      "Alphas: [0.37477881 0.17030652 0.09128953 0.36362514]\n"
     ]
    }
   ],
   "source": [
    "print(EM(data, 4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T14:13:49.867375Z",
     "start_time": "2019-04-09T14:13:27.921738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambdas: [ 48.61940969  91.16648483 106.59129791 187.00303894]\n",
      "Alphas: [0.54419492 0.0584109  0.03395274 0.36344144]\n"
     ]
    }
   ],
   "source": [
    "print(EM(data, 4, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T14:14:22.277966Z",
     "start_time": "2019-04-09T14:13:49.868321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambdas: [ 48.6069794   88.8900196  104.53356741 186.9945973 ]\n",
      "Alphas: [0.54384751 0.04643465 0.046205   0.36351284]\n"
     ]
    }
   ],
   "source": [
    "print(EM(data, 4, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T14:15:06.580191Z",
     "start_time": "2019-04-09T14:14:22.278963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambdas: [ 48.49218306  51.10126501  96.97258492 186.98114121]\n",
      "Alphas: [0.51237555 0.03258357 0.09141421 0.36362667]\n"
     ]
    }
   ],
   "source": [
    "print(EM(data, 4, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T14:16:01.212882Z",
     "start_time": "2019-04-09T14:15:06.581185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambdas: [ 48.61123158  89.68357381 105.26871846 186.9975237 ]\n",
      "Alphas: [0.54396808 0.05062697 0.04191686 0.36348809]\n"
     ]
    }
   ],
   "source": [
    "print(EM(data, 4, 20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
