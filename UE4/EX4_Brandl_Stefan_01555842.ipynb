{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Assignment 4: Logistic Regression</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Machine Learning: Theoretical Concepts, SS 2019</h2>\n",
    "\n",
    "<h3 style=\"color:rgb(0,120,170)\">Deadline: see Moodle</h3>\n",
    "Return this notebook with your code and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T16:19:55.515121Z",
     "start_time": "2019-05-03T16:19:54.451965Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "\n",
    "<h3 style=\"color:rgb(0,120,170)\"> Question 4.1 </h3>\n",
    "\n",
    "The coefficients $\\mathbf{w}$ of the Logistic Regression problem are usually estimated using the maximum likelihood method.\n",
    "This leads to the optimization problem \n",
    "\\begin{align*}\n",
    "\\min_{\\mathbf{w}} L &= \\min_{\\mathbf{w}} \\left(- \\sum_i y_i \\log \\sigma(\\mathbf{w}^T\\mathbf{x}_i) + (1-y_i) \\log (1-\\sigma(\\mathbf{w}^T \\mathbf{x}_i)) \\right)\\\\\n",
    "\\text{with} \\quad \\sigma(x) &= \\frac{1}{1+\\mathrm{e}^{-x}} \n",
    "\\end{align*}\n",
    "where $L = -\\ln \\mathcal{L}$ is the negative log-likelihood. Since there is no closed-form solution for $\\mathbf{w}$, an iterative method $-$ such as gradient descent $-$ must be applied in order to find $\\mathbf{w}$.  \n",
    "Calculate the gradient $\\frac{\\partial L}{\\partial \\mathbf{w}}$, which is needed to apply gradient descent.\n",
    "Hint: $\\frac{\\partial \\sigma(x)}{\\partial x} = \\sigma(x) \\cdot (1 - \\sigma(x))$.\n",
    "\n",
    "* <span style=\"color:rgb(0,120,170)\">**TODO:** Markdown, Latex</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(208,90,80)\">Answer 4.1:</h3>\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}} \\big( - \\sum_i^n y_i \\log \\sigma (\\mathbf{w}^T \\mathbf{x}_i) +  (1 - y_i) \\log ( 1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i))\\big) &= \\\\\n",
    "- \\sum_i^n \\frac{y_i \\sigma (\\mathbf{w}^T \\mathbf{x}_i)'}{\\sigma (\\mathbf{w}^T \\mathbf{x}_i)} + \\frac{(1 - y_i) (1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i))'}{1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i)} &= \\\\\n",
    "- \\sum_i^n \\frac{y_i (\\mathbf{w}^T \\mathbf{x}_i)' \\sigma(\\mathbf{w}^T \\mathbf{x}_i)(1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i))}{\\sigma (\\mathbf{w}^T \\mathbf{x}_i)} + \\frac{-(1 - y_i) (\\mathbf{w}^T \\mathbf{x}_i)'\\sigma(\\mathbf{w}^T \\mathbf{x}_i)(1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i))}{1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i)} &= \\\\\n",
    "- \\sum_i^n y_i (\\mathbf{w}^T \\mathbf{x}_i)' (1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i)) ~ - ~ (1 - y_i) (\\mathbf{w}^T \\mathbf{x}_i)'\\sigma(\\mathbf{w}^T \\mathbf{x}_i) &= \\\\\n",
    "- \\sum_i^n y_i \\mathbf{x}_i (1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i)) ~ - ~ (1 - y_i) \\mathbf{x}_i \\sigma(\\mathbf{w}^T \\mathbf{x}_i) &= \\\\\n",
    "- \\sum_i^n \\mathbf{x}_i \\big( y_i (1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i)) ~ - ~ (1 - y_i) \\sigma(\\mathbf{w}^T \\mathbf{x}_i) \\big) &= \\\\\n",
    "- \\sum_i^n \\mathbf{x}_i \\big( y_i - \\sigma(\\mathbf{w}^T \\mathbf{x}_i) \\big)\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\"> Question 4.2 </h3>\n",
    "\n",
    "Implement the formula for the gradient you found in Exercise 4.1. This means: implement a function \n",
    "`logistic_gradient(w, x, y)` that takes a parameter vector\n",
    "$\\mathbf{w}$, a data matrix $\\mathbf{X}$ and a label vector\n",
    "$\\mathbf{y}$ and returns the gradient $\\frac{\\partial L}{\\partial\n",
    "\\mathbf{w}}$.\n",
    "\n",
    "Test this function by implementing Gradient Checking. To do this, implement\n",
    "a function `numerical_gradient(w, x, y)` that takes the same parameters\n",
    "as `logistic_gradient`, but calculates the gradient numerically via the central difference quotient.\n",
    "\n",
    "Generate a random data matrix as well as random labels and a random weight\n",
    "vector and use them as input of both functions. Compare the outputs.\n",
    "\n",
    "* <span style=\"color:rgb(0,120,170)\">**TODO:** Markdown / Latex</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(208,90,80)\">Answer 4.2:</h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T16:19:55.530081Z",
     "start_time": "2019-05-03T16:19:55.518113Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def likelihood(w, X, y):\n",
    "    l = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        l -= y[i] * np.log(sigmoid(w.T @ X[i, :])) + (1 - y[i]) * np.log(1 - sigmoid(w.T @ X[i, :]))\n",
    "    return l\n",
    "\n",
    "def logistic_gradient(w, X, y):\n",
    "    s = np.zeros(X.shape[1])\n",
    "    for i in range(X.shape[0]):\n",
    "        s += X[i, :] * (y[i] - sigmoid(w.T @ X[i, :]))\n",
    "    return -s\n",
    "\n",
    "def numerical_gradient(w, X, y):\n",
    "    e = 1e-4\n",
    "    E = np.identity(len(w))\n",
    "    w_new = np.zeros_like(w)\n",
    "    for i in range(len(w)):\n",
    "        w_new[i] = (likelihood(w + e * E[i], X, y) - likelihood(w - e * E[i], X, y)) / (2 * e)\n",
    "    return w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T16:19:55.572006Z",
     "start_time": "2019-05-03T16:19:55.533590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic  gradient: [49.29409746 23.12707273 34.54810494]\n",
      "numerical gradient: [49.2940973  23.12707263 34.54810481]\n",
      "All values closer than 1e-05? True\n"
     ]
    }
   ],
   "source": [
    "def gen_w_X_y(size, n_dimensions):\n",
    "    w = np.random.rand(n_dimensions)\n",
    "    X = np.random.randint(-4, 4, (size, n_dimensions))\n",
    "    y = np.random.randint(0, 2, size)\n",
    "    return w, X, y\n",
    "\n",
    "def print_gradient(name, gradient, w, X, y):\n",
    "    g = gradient(w, X, y)\n",
    "    print(('%-9s' % name) + f' gradient: {g}')\n",
    "    return g\n",
    "\n",
    "w, X, y = gen_w_X_y(100, 3)\n",
    "g1 = print_gradient('logistic', logistic_gradient, w, X, y)\n",
    "g2 = print_gradient('numerical', numerical_gradient, w, X, y)\n",
    "\n",
    "tol = 1e-5\n",
    "print(f'All values closer than {tol}? {np.allclose(g1, g2, rtol=tol)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\"> Question 4.3 </h3>\n",
    "\n",
    "Consider the data sets `DataSet4a.csv` and `DataSet4b.csv`. \n",
    "Split each of these data sets into training set and test set ($50\\,\\%$ each).\n",
    "Then use Logistic Regression with Gradient Descent to compute classifiers on the training set and\n",
    "apply them to the test samples. Use randomly initialized weights, a learning rate of $10^{-4}$, and think of a good stopping criterion.\n",
    "\n",
    "Predict class `1` if the Logistic Regression returns $\\geq 0.5$ and `0` otherwise. \n",
    "Calculate Accuracy and Balanced Accuracy on the test samples.\n",
    "\n",
    "Further provide ROC (receiver operating characteristic) curves of the classifiers on the test samples and compute the AUC (area under curve) value of these curves. Therefore the functions `roc_curve` and `auc` from `sklearn.metrics` might be useful.\n",
    "\n",
    "\n",
    "* <span style=\"color:rgb(0,120,170)\">**TODO:** Markdown / Latex</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(208,90,80)\">Answer 4.3:</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T16:19:55.588962Z",
     "start_time": "2019-05-03T16:19:55.574998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 2) (350, 2)\n"
     ]
    }
   ],
   "source": [
    "A = np.genfromtxt('DataSet4a.csv', delimiter=',', skip_header=1)\n",
    "B = np.genfromtxt('DataSet4b.csv', delimiter=',', skip_header=1)\n",
    "\n",
    "# Converting class -1 to 0\n",
    "X_a, y_a = A[:, :-1], (A[:, -1] > 0).astype(int)\n",
    "X_b, y_b = B[:, :-1], (B[:, -1] > 0).astype(int)\n",
    "\n",
    "print(X_a.shape, X_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T16:19:55.607911Z",
     "start_time": "2019-05-03T16:19:55.591955Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_a_train, X_a_test, y_a_train, y_a_test = train_test_split(X_a, y_a, test_size=.5)\n",
    "X_b_train, X_b_test, y_b_train, y_b_test = train_test_split(X_b, y_b, test_size=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T16:20:16.786892Z",
     "start_time": "2019-05-03T16:19:55.610904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached in iteration 6814\n",
      "Convergence reached in iteration 7118\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(w, X, y, learning_rate=1e-4, max_iter=int(1e4)):\n",
    "    w_curr = w.copy()\n",
    "    for i in range(max_iter):\n",
    "        w_prev = w_curr\n",
    "        w_curr = w_prev - learning_rate * logistic_gradient(w_prev, X, y)\n",
    "        if np.allclose(w_curr, w_prev, rtol=1e-4):\n",
    "            print(f'Convergence reached in iteration {i}')\n",
    "            return w_curr\n",
    "    else:\n",
    "        print(f'Regression did not converge after {max_iter} iterations')\n",
    "        return w_curr\n",
    "    \n",
    "w_a = logistic_regression(np.random.rand(X_a.shape[1]), X_a_train, y_a_train)\n",
    "w_b = logistic_regression(np.random.rand(X_b.shape[1]), X_b_train, y_b_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T16:20:16.810343Z",
     "start_time": "2019-05-03T16:20:16.789887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual    : [0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0\n",
      " 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0]\n",
      "Prediction: [0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0.]\n",
      "Accuracy          of dataset a: 0.8333333333333334\n",
      "\n",
      "Actual    : [0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0\n",
      " 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0]\n",
      "Prediction: [0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0.]\n",
      "Balanced Accuracy of dataset a: 0.8348214285714286\n",
      "\n",
      "Actual    : [0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0\n",
      " 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1\n",
      " 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0\n",
      " 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 0]\n",
      "Prediction: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0.]\n",
      "Accuracy          of dataset b: 0.7542857142857143\n",
      "\n",
      "Actual    : [0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0\n",
      " 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1\n",
      " 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0\n",
      " 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 0]\n",
      "Prediction: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0.]\n",
      "Balanced Accuracy of dataset b: 0.5957463630613535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def classify(X, w):\n",
    "    return np.asarray([(w.T @ x_i >= 0.5, w.T @ x_i) for x_i in X]).astype(float)\n",
    "\n",
    "def accuracy(pred, test):\n",
    "    return (pred == test).sum() / len(pred)\n",
    "\n",
    "def balanced_accuracy(pred, test):\n",
    "    positive = ((pred == 1) & (test == 1)).sum()\n",
    "    negative = ((pred == 0) & (test == 0)).sum()\n",
    "    return (positive / test.sum() + negative / (len(test) - test.sum())) / 2\n",
    "\n",
    "pred_a = classify(X_a_test, w_a)\n",
    "pred_b = classify(X_b_test, w_b)\n",
    "\n",
    "def print_accuracy(name, dataset, fun, pred, test):\n",
    "    print(f'Actual    : {test}')\n",
    "    print(f'Prediction: {pred}')\n",
    "    print(('%-17s' % name) + f' of dataset {dataset}: {fun(pred, test)}\\n')\n",
    "    \n",
    "print_accuracy('Accuracy', 'a', accuracy, pred_a[:, 0], y_a_test)\n",
    "print_accuracy('Balanced Accuracy', 'a', balanced_accuracy, pred_a[:, 0], y_a_test)\n",
    "print_accuracy('Accuracy', 'b', accuracy, pred_b[:, 0], y_b_test)\n",
    "print_accuracy('Balanced Accuracy', 'b', balanced_accuracy, pred_b[:, 0], y_b_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T16:20:17.081135Z",
     "start_time": "2019-05-03T16:20:16.813848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr_a, tpr_a, _ = roc_curve(y_a_test, pred_a[:, 1])\n",
    "fpr_b, tpr_b, _ = roc_curve(y_b_test, pred_b[:, 1])\n",
    "\n",
    "auc_a = auc(fpr_a, tpr_a)\n",
    "auc_b = auc(fpr_b, tpr_b)\n",
    "\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "plt.title('ROC')\n",
    "plt.plot(fpr_a, tpr_a, color='blue', alpha=0.5, label=f'A | AUC={\"%.4f\" % auc_a}')\n",
    "plt.plot(fpr_b, tpr_b, color='red', alpha=0.5, label=f'B | AUC={\"%.4f\" % auc_b}')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\"> Question 4.4 </h3>\n",
    "\n",
    "We consider again the objective of Logistic Regression, but now in a more abstract form, i.e. we set $net_i=\\mathbf{w}^T\\mathbf{x}_i$ and further $a_i=\\sigma(net_i)$. Then the ojective can be  written as:\n",
    "\\begin{align*}\n",
    "\\min_{\\mathbf{w}} \\quad L &= - \\sum_i y_i \\log \\sigma(net_i) + (1-y_i) \\log (1-\\sigma(net_i))\\\\\n",
    "&= - \\sum_i y_i \\log a_i + (1-y_i) \\log (1-a_i)\\\\\n",
    "\\text{with} \\quad \\sigma(x) &= \\frac{1}{1+\\mathrm{e}^{-x}} \n",
    "\\end{align*}\n",
    "\n",
    "Now, try to find $\\frac{\\partial L}{\\partial a_i}$ and $\\frac{\\partial L}{\\partial net_i}$. Simplify the terms as much as you can (especially $\\frac{\\partial L}{\\partial net_i}$) and  express it only in terms of $y_i$, $a_i$ and basic arithmetic operations $(+,-,*,/)$. Which values are possible for $\\frac{\\partial L}{\\partial net_i}$? Discuss what the sign and the interval of possible values of $\\frac{\\partial L}{\\partial net_i}$ is for true positives, true negatives, false postives, and false negatives, where classification is done with the criterion $a_i \\geq 0.5$. \n",
    "\n",
    "Consider the hint of Question 4.1.\n",
    "\n",
    "* <span style=\"color:rgb(0,120,170)\">**TODO:** Markdown, Latex</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(208,90,80)\">Answer 4.4:</h3>\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial net_i} &= -\\sum_i y_i \\log \\sigma(net_i) + (1 - y_i)\\log(1 - \\sigma(net_i)) \\\\\n",
    "&= -\\sum_i \\frac{y_i \\sigma(net_i) (1 - \\sigma(net_i))}{\\sigma(net_i)} - \\frac{(1 - y_i)\\sigma(net_i)(1 - \\sigma(net_i))}{1 - \\sigma(net_i)} \\\\\n",
    "&= -\\sum_i y_i(1 - \\sigma(net_i)) - (1 - y_i)\\sigma(net_i) \\\\\n",
    "&= -\\sum_i y_i - y_i\\sigma(net_i) - \\sigma(net_i) + y_i\\sigma(net_i) \\\\\n",
    "&= -\\sum_i y_i - \\sigma(net_i) \\\\\n",
    "&= \\sum_i \\sigma(net_i) - y_i\n",
    "\\end{align*}\n",
    "\n",
    "---\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial a_i} &= -\\sum_i y_i \\log a_i + (1 - y_i)\\log(1 - a_i) \\\\\n",
    "&= -\\sum_i \\frac{y_i}{a_i} - \\frac{1 - y_i}{1 - a_i} \\\\\n",
    "&= -\\sum_i \\frac{y_i - y_i a_i}{a_i(1 - a_i)} - \\frac{a_i - y_i a_i}{a_i(1 - a_i)} \\\\\n",
    "&= -\\sum_i \\frac{y_i - y_i a_i - a_i + y_i a_i}{a_i(1 - a_i)} \\\\\n",
    "&= -\\sum_i \\frac{y_i - a_i}{a_i(1 - a_i)} \\\\\n",
    "&= \\sum_i \\frac{a_i - y_i}{a_i(1 - a_i)}\n",
    "\\end{align*}\n",
    "\n",
    "---\n",
    "\n",
    "With respect to \n",
    "\\begin{align*}\n",
    "-\\sum_i \\frac{y_i - a_i}{a_i(1 - a_i)}\n",
    "\\end{align*}\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "      <th/>\n",
    "      <th> True Positive </th>\n",
    "      <th> False Negative </th>\n",
    "      <th> True Negative </th>\n",
    "      <th> False Positive </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> $y_i$ </td>\n",
    "    <td> 1 </td>\n",
    "    <td> 1 </td>\n",
    "    <td> 0 </td>\n",
    "    <td> 0 </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> $actual$ </td>\n",
    "    <td> 1 </td>\n",
    "    <td> 0 </td>\n",
    "    <td> 0 </td>\n",
    "    <td> 1 </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> $y_i$ $-actual$ </td>\n",
    "    <td> 0 </td>\n",
    "    <td> 1 </td>\n",
    "    <td> 0 </td>\n",
    "    <td> -1 </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> $a_i$ </td>\n",
    "    <td> $(0.5, 1)$ </td>\n",
    "    <td> $(0, 0.5)$ </td>\n",
    "    <td> $(0, 0.5)$ </td>\n",
    "    <td> $(0.5, 1)$ </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> $y_i - a_i$ </td>\n",
    "    <td> $(0, 0.5)$ </td>\n",
    "    <td> $(0.5, 1)$ </td>\n",
    "    <td> $(-0.5, 0)$ </td>\n",
    "    <td> $(-1, -0.5)$ </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> Sign (see $y_i - a_i$) </td>\n",
    "    <td> + </td>\n",
    "    <td> + </td>\n",
    "    <td> - </td>\n",
    "    <td> - </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
