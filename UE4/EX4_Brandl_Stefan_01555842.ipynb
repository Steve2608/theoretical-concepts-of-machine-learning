{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Assignment 4: Logistic Regression</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Machine Learning: Theoretical Concepts, SS 2019</h2>\n",
    "\n",
    "<h3 style=\"color:rgb(0,120,170)\">Deadline: see Moodle</h3>\n",
    "Return this notebook with your code and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T16:19:55.515121Z",
     "start_time": "2019-05-03T16:19:54.451965Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Callable, Tuple\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "\n",
    "<h3 style=\"color:rgb(0,120,170)\"> Question 4.1 </h3>\n",
    "\n",
    "The coefficients $\\mathbf{w}$ of the Logistic Regression problem are usually estimated using the maximum likelihood method.\n",
    "This leads to the optimization problem \n",
    "\\begin{align*}\n",
    "\\min_{\\mathbf{w}} L &= \\min_{\\mathbf{w}} \\left(- \\sum_i y_i \\log \\sigma(\\mathbf{w}^T\\mathbf{x}_i) + (1-y_i) \\log (1-\\sigma(\\mathbf{w}^T \\mathbf{x}_i)) \\right)\\\\\n",
    "\\text{with} \\quad \\sigma(x) &= \\frac{1}{1+\\mathrm{e}^{-x}} \n",
    "\\end{align*}\n",
    "where $L = -\\ln \\mathcal{L}$ is the negative log-likelihood. Since there is no closed-form solution for $\\mathbf{w}$, an iterative method $-$ such as gradient descent $-$ must be applied in order to find $\\mathbf{w}$.  \n",
    "Calculate the gradient $\\frac{\\partial L}{\\partial \\mathbf{w}}$, which is needed to apply gradient descent.\n",
    "Hint: $\\frac{\\partial \\sigma(x)}{\\partial x} = \\sigma(x) \\cdot (1 - \\sigma(x))$.\n",
    "\n",
    "* <span style=\"color:rgb(0,120,170)\">**TODO:** Markdown, Latex</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(208,90,80)\">Answer 4.1:</h3>\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}} \\big( - \\sum_i^n y_i \\log \\sigma (\\mathbf{w}^T \\mathbf{x}_i) +  (1 - y_i) \\log ( 1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i))\\big) &= \\\\\n",
    "- \\sum_i^n \\frac{y_i \\sigma (\\mathbf{w}^T \\mathbf{x}_i)'}{\\sigma (\\mathbf{w}^T \\mathbf{x}_i)} + \\frac{(1 - y_i) (1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i))'}{1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i)} &= \\\\\n",
    "- \\sum_i^n \\frac{y_i (\\mathbf{w}^T \\mathbf{x}_i)' \\sigma(\\mathbf{w}^T \\mathbf{x}_i)(1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i))}{\\sigma (\\mathbf{w}^T \\mathbf{x}_i)} + \\frac{-(1 - y_i) (\\mathbf{w}^T \\mathbf{x}_i)'\\sigma(\\mathbf{w}^T \\mathbf{x}_i)(1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i))}{1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i)} &= \\\\\n",
    "- \\sum_i^n y_i (\\mathbf{w}^T \\mathbf{x}_i)' (1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i)) ~ - ~ (1 - y_i) (\\mathbf{w}^T \\mathbf{x}_i)'\\sigma(\\mathbf{w}^T \\mathbf{x}_i) &= \\\\\n",
    "- \\sum_i^n y_i \\mathbf{x}_i (1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i)) ~ - ~ (1 - y_i) \\mathbf{x}_i \\sigma(\\mathbf{w}^T \\mathbf{x}_i) &= \\\\\n",
    "- \\sum_i^n \\mathbf{x}_i \\big( y_i (1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i)) ~ - ~ (1 - y_i) \\sigma(\\mathbf{w}^T \\mathbf{x}_i) \\big) &= \\\\\n",
    "- \\sum_i^n \\mathbf{x}_i \\big( y_i - \\sigma(\\mathbf{w}^T \\mathbf{x}_i) \\big)\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\"> Question 4.2 </h3>\n",
    "\n",
    "Implement the formula for the gradient you found in Exercise 4.1. This means: implement a function \n",
    "`logistic_gradient(w, x, y)` that takes a parameter vector\n",
    "$\\mathbf{w}$, a data matrix $\\mathbf{X}$ and a label vector\n",
    "$\\mathbf{y}$ and returns the gradient $\\frac{\\partial L}{\\partial\n",
    "\\mathbf{w}}$.\n",
    "\n",
    "Test this function by implementing Gradient Checking. To do this, implement\n",
    "a function `numerical_gradient(w, x, y)` that takes the same parameters\n",
    "as `logistic_gradient`, but calculates the gradient numerically via the central difference quotient.\n",
    "\n",
    "Generate a random data matrix as well as random labels and a random weight\n",
    "vector and use them as input of both functions. Compare the outputs.\n",
    "\n",
    "* <span style=\"color:rgb(0,120,170)\">**TODO:** Markdown / Latex</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(208,90,80)\">Answer 4.2:</h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T16:19:55.530081Z",
     "start_time": "2019-05-03T16:19:55.518113Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def likelihood(w: np.ndarray, X: np.ndarray, y: np.ndarray) -> float:\n",
    "    l = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        s = sigmoid(w.T @ X[i, :])\n",
    "        l += y[i] * np.log(s) + (1 - y[i]) * np.log(1 - s)\n",
    "    return -l\n",
    "\n",
    "\n",
    "def logistic_gradient(w: np.ndarray, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    return - (X.T @ (y - sigmoid(X @ w)))\n",
    "    \n",
    "\n",
    "def numerical_gradient(w: np.ndarray, X: np.ndarray, y: np.ndarray, e: float = 1e-4) -> np.ndarray:\n",
    "    E = np.eye(len(w))\n",
    "    w_new = np.zeros_like(w)\n",
    "    for i in range(len(w)):\n",
    "        w_new[i] = (likelihood(w + e * E[i], X, y) - likelihood(w - e * E[i], X, y)) / (2 * e)\n",
    "    return w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T16:19:55.572006Z",
     "start_time": "2019-05-03T16:19:55.533590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic  gradient: [31.82803752 53.15022931 29.13594639]\n",
      "numerical gradient: [31.82803738 53.15022913 29.1359463 ]\n",
      "All values closer than 1e-05? True\n"
     ]
    }
   ],
   "source": [
    "def gen_w_X_y(size: int, n_dimensions: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    w = np.random.rand(n_dimensions)\n",
    "    X = np.random.randint(-4, 4, (size, n_dimensions))\n",
    "    y = np.random.randint(0, 2, size)\n",
    "    return w, X, y\n",
    "    \n",
    "\n",
    "def print_gradient(name: str, gradient: Callable[[np.ndarray, np.ndarray, np.ndarray], np.ndarray],\n",
    "                   w: np.ndarray, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    g = gradient(w, X, y)\n",
    "    print(f'{name:9s} gradient: {g}')\n",
    "    return g\n",
    "\n",
    "\n",
    "w, X, y = gen_w_X_y(100, 3)\n",
    "g1 = print_gradient('logistic', logistic_gradient, w, X, y)\n",
    "g2 = print_gradient('numerical', numerical_gradient, w, X, y)\n",
    "\n",
    "tol = 1e-5\n",
    "print(f'All values closer than {tol}? {np.allclose(g1, g2, atol=tol)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\"> Question 4.3 </h3>\n",
    "\n",
    "Consider the data sets `DataSet4a.csv` and `DataSet4b.csv`. \n",
    "Split each of these data sets into training set and test set ($50\\,\\%$ each).\n",
    "Then use Logistic Regression with Gradient Descent to compute classifiers on the training set and\n",
    "apply them to the test samples. Use randomly initialized weights, a learning rate of $10^{-4}$, and think of a good stopping criterion.\n",
    "\n",
    "Predict class `1` if the Logistic Regression returns $\\geq 0.5$ and `0` otherwise. \n",
    "Calculate Accuracy and Balanced Accuracy on the test samples.\n",
    "\n",
    "Further provide ROC (receiver operating characteristic) curves of the classifiers on the test samples and compute the AUC (area under curve) value of these curves. Therefore the functions `roc_curve` and `auc` from `sklearn.metrics` might be useful.\n",
    "\n",
    "\n",
    "* <span style=\"color:rgb(0,120,170)\">**TODO:** Markdown / Latex</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(208,90,80)\">Answer 4.3:</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T16:19:55.588962Z",
     "start_time": "2019-05-03T16:19:55.574998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 2) (350, 2)\n"
     ]
    }
   ],
   "source": [
    "A = np.genfromtxt('DataSet4a.csv', delimiter=',', skip_header=1)\n",
    "B = np.genfromtxt('DataSet4b.csv', delimiter=',', skip_header=1)\n",
    "\n",
    "# Converting class -1 to 0\n",
    "X_a, y_a = A[:, :-1], (A[:, -1] > 0).astype(int)\n",
    "X_b, y_b = B[:, :-1], (B[:, -1] > 0).astype(int)\n",
    "\n",
    "print(X_a.shape, X_b.shape)\n",
    "\n",
    "X_a_train, X_a_test, y_a_train, y_a_test = train_test_split(X_a, y_a, test_size=.5)\n",
    "X_b_train, X_b_test, y_b_train, y_b_test = train_test_split(X_b, y_b, test_size=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T16:20:16.786892Z",
     "start_time": "2019-05-03T16:19:55.610904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached in iteration 7843\n",
      "Convergence reached in iteration 6586\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(w: np.ndarray, X: np.ndarray, y: np.ndarray, \n",
    "                        learning_rate: float = 1e-4, max_iter: int = int(1e4)) -> np.ndarray:\n",
    "    w_curr = w.copy()\n",
    "    for i in range(max_iter):\n",
    "        w_prev = w_curr\n",
    "        w_curr = w_prev - learning_rate * logistic_gradient(w_prev, X, y)\n",
    "        if np.allclose(w_curr, w_prev, rtol=1e-4):\n",
    "            print(f'Convergence reached in iteration {i}')\n",
    "            return w_curr\n",
    "    else:\n",
    "        print(f'Regression did not converge after {max_iter} iterations')\n",
    "        return w_curr\n",
    "    \n",
    "w_a = logistic_regression(np.random.rand(X_a.shape[1]), X_a_train, y_a_train)\n",
    "w_b = logistic_regression(np.random.rand(X_b.shape[1]), X_b_train, y_b_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T16:20:16.810343Z",
     "start_time": "2019-05-03T16:20:16.789887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual    : [0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1\n",
      " 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0]\n",
      "Prediction: [0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      "Accuracy          of dataset a: 0.7833333333333333\n",
      "\n",
      "Actual    : [0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1\n",
      " 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0]\n",
      "Prediction: [0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      "Balanced Accuracy of dataset a: 0.7929292929292929\n",
      "\n",
      "Actual    : [0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0\n",
      " 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
      " 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "Prediction: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "Accuracy          of dataset b: 0.7657142857142857\n",
      "\n",
      "Actual    : [0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 0\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0\n",
      " 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
      " 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      "Prediction: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "Balanced Accuracy of dataset b: 0.5793963254593176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def classify(X: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "    r = X @ w\n",
    "    return np.asarray([r >= 0.5, r]).T.astype(float)\n",
    "\n",
    "\n",
    "def accuracy(pred: np.ndarray, test: np.ndarray) -> float:\n",
    "    return np.sum(pred == test) / len(pred)\n",
    "\n",
    "\n",
    "def balanced_accuracy(pred: np.ndarray, test: np.ndarray) -> float:\n",
    "    positive = np.sum((pred == 1) & (test == 1))\n",
    "    negative = np.sum((pred == 0) & (test == 0))\n",
    "    return (positive / test.sum() + negative / (len(test) - test.sum())) / 2\n",
    "\n",
    "\n",
    "def print_accuracy(name: str, dataset: str, fun: Callable[[np.ndarray, np.ndarray], float],\n",
    "                   pred: np.ndarray, test: np.ndarray):\n",
    "    print(f'Actual    : {test}')\n",
    "    print(f'Prediction: {pred}')\n",
    "    print(f'{name:17s} of dataset {dataset}: {fun(pred, test)}', end='\\n\\n')\n",
    "\n",
    "\n",
    "pred_a = classify(X_a_test, w_a)\n",
    "pred_b = classify(X_b_test, w_b)\n",
    "\n",
    "print_accuracy('Accuracy', 'a', accuracy, pred_a[:, 0], y_a_test)\n",
    "print_accuracy('Balanced Accuracy', 'a', balanced_accuracy, pred_a[:, 0], y_a_test)\n",
    "print_accuracy('Accuracy', 'b', accuracy, pred_b[:, 0], y_b_test)\n",
    "print_accuracy('Balanced Accuracy', 'b', balanced_accuracy, pred_b[:, 0], y_b_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T16:20:17.081135Z",
     "start_time": "2019-05-03T16:20:16.813848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr_a, tpr_a, _ = roc_curve(y_a_test, pred_a[:, 1])\n",
    "fpr_b, tpr_b, _ = roc_curve(y_b_test, pred_b[:, 1])\n",
    "\n",
    "auc_a = auc(fpr_a, tpr_a)\n",
    "auc_b = auc(fpr_b, tpr_b)\n",
    "\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "plt.title('ROC')\n",
    "plt.plot(fpr_a, tpr_a, color='blue', alpha=0.5, label=f'A | AUC={auc_a:.3f}')\n",
    "plt.plot(fpr_b, tpr_b, color='red', alpha=0.5, label=f'B | AUC={auc_b:.3f}')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\"> Question 4.4 </h3>\n",
    "\n",
    "We consider again the objective of Logistic Regression, but now in a more abstract form, i.e. we set $net_i=\\mathbf{w}^T\\mathbf{x}_i$ and further $a_i=\\sigma(net_i)$. Then the ojective can be  written as:\n",
    "\\begin{align*}\n",
    "\\min_{\\mathbf{w}} \\quad L &= - \\sum_i y_i \\log \\sigma(net_i) + (1-y_i) \\log (1-\\sigma(net_i))\\\\\n",
    "&= - \\sum_i y_i \\log a_i + (1-y_i) \\log (1-a_i)\\\\\n",
    "\\text{with} \\quad \\sigma(x) &= \\frac{1}{1+\\mathrm{e}^{-x}} \n",
    "\\end{align*}\n",
    "\n",
    "Now, try to find $\\frac{\\partial L}{\\partial a_i}$ and $\\frac{\\partial L}{\\partial net_i}$. Simplify the terms as much as you can (especially $\\frac{\\partial L}{\\partial net_i}$) and  express it only in terms of $y_i$, $a_i$ and basic arithmetic operations $(+,-,*,/)$. Which values are possible for $\\frac{\\partial L}{\\partial net_i}$? Discuss what the sign and the interval of possible values of $\\frac{\\partial L}{\\partial net_i}$ is for true positives, true negatives, false postives, and false negatives, where classification is done with the criterion $a_i \\geq 0.5$. \n",
    "\n",
    "Consider the hint of Question 4.1.\n",
    "\n",
    "* <span style=\"color:rgb(0,120,170)\">**TODO:** Markdown, Latex</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(208,90,80)\">Answer 4.4:</h3>\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial net_i} &= -\\sum_i y_i \\log \\sigma(net_i) + (1 - y_i)\\log(1 - \\sigma(net_i)) \\\\\n",
    "&= -\\sum_i \\frac{y_i \\sigma(net_i) (1 - \\sigma(net_i))}{\\sigma(net_i)} - \\frac{(1 - y_i)\\sigma(net_i)(1 - \\sigma(net_i))}{1 - \\sigma(net_i)} \\\\\n",
    "&= -\\sum_i y_i(1 - \\sigma(net_i)) - (1 - y_i)\\sigma(net_i) \\\\\n",
    "&= -\\sum_i y_i - y_i\\sigma(net_i) - \\sigma(net_i) + y_i\\sigma(net_i) \\\\\n",
    "&= -\\sum_i y_i - \\sigma(net_i) \\\\\n",
    "&= \\sum_i \\sigma(net_i) - y_i\n",
    "\\end{align*}\n",
    "\n",
    "---\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial a_i} &= -\\sum_i y_i \\log a_i + (1 - y_i)\\log(1 - a_i) \\\\\n",
    "&= -\\sum_i \\frac{y_i}{a_i} - \\frac{1 - y_i}{1 - a_i} \\\\\n",
    "&= -\\sum_i \\frac{y_i - y_i a_i}{a_i(1 - a_i)} - \\frac{a_i - y_i a_i}{a_i(1 - a_i)} \\\\\n",
    "&= -\\sum_i \\frac{y_i - y_i a_i - a_i + y_i a_i}{a_i(1 - a_i)} \\\\\n",
    "&= -\\sum_i \\frac{y_i - a_i}{a_i(1 - a_i)} \\\\\n",
    "&= \\sum_i \\frac{a_i - y_i}{a_i(1 - a_i)}\n",
    "\\end{align*}\n",
    "\n",
    "---\n",
    "\n",
    "With respect to \n",
    "\\begin{align*}\n",
    "-\\sum_i \\frac{y_i - a_i}{a_i(1 - a_i)}\n",
    "\\end{align*}\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "      <th/>\n",
    "      <th> True Positive </th>\n",
    "      <th> False Negative </th>\n",
    "      <th> True Negative </th>\n",
    "      <th> False Positive </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> $y_i$ </td>\n",
    "    <td> 1 </td>\n",
    "    <td> 1 </td>\n",
    "    <td> 0 </td>\n",
    "    <td> 0 </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> $actual$ </td>\n",
    "    <td> 1 </td>\n",
    "    <td> 0 </td>\n",
    "    <td> 0 </td>\n",
    "    <td> 1 </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> $y_i$ $-actual$ </td>\n",
    "    <td> 0 </td>\n",
    "    <td> 1 </td>\n",
    "    <td> 0 </td>\n",
    "    <td> -1 </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> $a_i$ </td>\n",
    "    <td> $(0.5, 1)$ </td>\n",
    "    <td> $(0, 0.5)$ </td>\n",
    "    <td> $(0, 0.5)$ </td>\n",
    "    <td> $(0.5, 1)$ </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> $y_i - a_i$ </td>\n",
    "    <td> $(0, 0.5)$ </td>\n",
    "    <td> $(0.5, 1)$ </td>\n",
    "    <td> $(-0.5, 0)$ </td>\n",
    "    <td> $(-1, -0.5)$ </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> Sign (see $y_i - a_i$) </td>\n",
    "    <td> + </td>\n",
    "    <td> + </td>\n",
    "    <td> - </td>\n",
    "    <td> - </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
